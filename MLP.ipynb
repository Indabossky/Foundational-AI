{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Vehicle MPG Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows removed: 6\n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "import pandas as pd\n",
    "\n",
    "vehicle_mpg = fetch_ucirepo(id=9)\n",
    "\n",
    "X = vehicle_mpg.data.features\n",
    "y = vehicle_mpg.data.targets\n",
    "\n",
    "data = pd.concat([X, y], axis=1)\n",
    "\n",
    "cleaned_data = data.dropna()\n",
    "\n",
    "X = cleaned_data.iloc[:, :-1]\n",
    "y = cleaned_data.iloc[:, -1]\n",
    "\n",
    "rows_removed = data.shape[0] - cleaned_data.shape[0]\n",
    "print(f\"Rows removed: {rows_removed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting and Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_leftover, y_train, y_leftover = train_test_split(X, y, test_size=0.3, random_state=42, shuffle = True)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_leftover, y_leftover, test_size=0.5, random_state=42, shuffle = True)\n",
    "\n",
    "X_mean = X_train.mean(axis=0)\n",
    "X_std = X_train.std(axis=0)\n",
    "\n",
    "X_train = (X_train - X_mean) / X_std\n",
    "X_val = (X_val - X_mean) / X_std\n",
    "X_test = (X_test - X_mean) / X_std\n",
    "\n",
    "y_mean = y_train.mean()\n",
    "y_std = y_train.std()\n",
    "\n",
    "y_train = (y_train - y_mean) / y_std\n",
    "y_val = (y_val - y_mean) / y_std\n",
    "y_test = (y_test - y_mean) / y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def batch_generator(train_x, train_y, batch_size):\n",
    "    \"\"\"\n",
    "    Generator that yields batches of train_x and train_y.\n",
    "\n",
    "    :param train_x (np.ndarray): Input features of shape (n, f).\n",
    "    :param train_y (np.ndarray): Target values of shape (n, q).\n",
    "    :param batch_size (int): The size of each batch.\n",
    "\n",
    "    :return tuple: (batch_x, batch_y) where batch_x has shape (B, f) and batch_y has shape (B, q). The last batch may be smaller.\n",
    "    \"\"\"\n",
    "    assert len(train_x) == len(train_y), \"Number of samples in X and y do not match.\"\n",
    "\n",
    "    batch_x = np.array_split(train_x, math.ceil(len(train_x) / batch_size), axis=0)\n",
    "    batch_y = np.array_split(train_y, math.ceil(len(train_y) / batch_size), axis=0)\n",
    "    return batch_x, batch_y\n",
    "\n",
    "\n",
    "class ActivationFunction(ABC):\n",
    "    @abstractmethod\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Computes the output of the activation function, evaluated on x\n",
    "\n",
    "        Input args may differ in the case of softmax\n",
    "\n",
    "        :param x (np.ndarray): input\n",
    "        :return: output of the activation function\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Computes the derivative of the activation function, evaluated on x\n",
    "        :param x (np.ndarray): input\n",
    "        :return: activation function's derivative at x\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class Sigmoid(ActivationFunction):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        sigma_x = 1 / (1 + np.exp(-x))\n",
    "        return sigma_x\n",
    "    \n",
    "    def derivative(self):\n",
    "        return (sigma_x)(1-sigma_x)           \n",
    "\n",
    "class Tanh(ActivationFunction):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        tanh_x = (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "        return tanh_x\n",
    "    \n",
    "    def deriative(self):\n",
    "        return 1 - (tanh_x)**2\n",
    "\n",
    "class Relu(ActivationFunction):\n",
    "    def __init__(self):\n",
    "        pass    \n",
    "\n",
    "    def forward(self, x):\n",
    "        return np.max(0, x)\n",
    "    \n",
    "    def derivative(self, x):\n",
    "        return np.ones_like(x)\n",
    "    \n",
    "\n",
    "class Softmax(ActivationFunction):\n",
    "    def __init__(self):\n",
    "        pass    \n",
    "\n",
    "    def forward(self, x):\n",
    "        return (np.exp(x))/(np.sum(x))\n",
    "    \n",
    "    def derivative(self):\n",
    "        pass\n",
    "\n",
    "class Linear(ActivationFunction):\n",
    "    def __init__(self):\n",
    "     pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "    def derivative(self, x):\n",
    "        return np.ones_like(x)\n",
    "\n",
    "    \n",
    "\n",
    "class LossFunction(ABC):\n",
    "    @abstractmethod\n",
    "    def loss(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def derivative(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def delta(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "\n",
    "class SquaredError(LossFunction):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def loss(self, y_true, y_pred):\n",
    "        return 1/2 * np.mean(np.square(y_pred - y_true.flatten()))\n",
    "    \n",
    "    def derivative(self, activ_func_deriv, prev_output, next_delta, weight):\n",
    "        dL_dW = np.dot(prev_output.T, np.multiply(next_delta, activ_func_deriv))\n",
    "        dL_db = np.sum(np.multiply(next_delta, activ_func_deriv))\n",
    "        delta = np.dot(np.multiply(next_delta, activ_func_deriv),weight)\n",
    "        return dL_dW, dL_db, delta\n",
    "\n",
    "class CrossEntropy(LossFunction):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, fan_in: int, fan_out: int, activation_function: ActivationFunction, loss_function: LossFunction):\n",
    "        \"\"\"\n",
    "        Initializes a layer of neurons\n",
    "\n",
    "        :param fan_in: number of neurons in previous (presynpatic) layer\n",
    "        :param fan_out: number of neurons in this layer\n",
    "        :param activation_function: instance of an ActivationFunction\n",
    "        \"\"\"\n",
    "        self.fan_in = fan_in\n",
    "        self.fan_out = fan_out\n",
    "        self.activation_function = activation_function\n",
    "        self.loss_function = loss_function\n",
    "\n",
    "        # this will store the activations (forward prop)\n",
    "        self.activations = None\n",
    "        # this will store the delta term (dL_dPhi, backward prop)\n",
    "        self.delta = None\n",
    "\n",
    "        # Initialize weights and biaes\n",
    "        self.W = None  # weights\n",
    "        self.b = None  # biases\n",
    "\n",
    "    def predict(self, x: np.ndarray):\n",
    "       return self.W @ x.T + self.b\n",
    "\n",
    "    def forward(self, h: np.ndarray):\n",
    "        \"\"\"\n",
    "        Computes the activations for this layer\n",
    "\n",
    "        :param h: input to layer\n",
    "        :return: layer activations\n",
    "        \"\"\"\n",
    "        self.activations = self.activation_function.forward(h)\n",
    "\n",
    "        return self.activations\n",
    "\n",
    "    def backward(self, h: np.ndarray, delta: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Apply backpropagation to this layer and return the weight and bias gradients\n",
    "\n",
    "        :param h: input to this layer\n",
    "        :param delta: delta term from layer above\n",
    "        :return: (weight gradients, bias gradients)\n",
    "        \"\"\"\n",
    "        dL_dW = self.loss_function.derivative()\n",
    "        dL_db = None\n",
    "        self.delta = None\n",
    "        return dL_dW, dL_db\n",
    "\n",
    "\n",
    "class MultilayerPerceptron:\n",
    "    def __init__(self, layers: Tuple[Layer]):\n",
    "        \"\"\"\n",
    "        Create a multilayer perceptron (densely connected multilayer neural network)\n",
    "        :param layers: list or Tuple of layers\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        This takes the network input and computes the network output (forward propagation)\n",
    "        :param x: network input\n",
    "        :return: network output\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            current_layer = Layer()\n",
    "            y_pred = current_layer.predict(x)\n",
    "            activation = current_layer.forward(y_pred)\n",
    "            x = activation\n",
    "            \n",
    "\n",
    "        return None\n",
    "\n",
    "    def backward(self, loss_grad: np.ndarray, input_data: np.ndarray) -> Tuple[list, list]:\n",
    "        \"\"\"\n",
    "        Applies backpropagation to compute the gradients of the weights and biases for all layers in the network\n",
    "        :param loss_grad: gradient of the loss function\n",
    "        :param input_data: network's input data\n",
    "        :return: (List of weight gradients for all layers, List of bias gradients for all layers)\n",
    "        \"\"\"\n",
    "        dl_dw_all = []\n",
    "        dl_db_all = []\n",
    "\n",
    "\n",
    "        return None, None\n",
    "\n",
    "    def train(self, train_x: np.ndarray, train_y: np.ndarray, val_x: np.ndarray, val_y: np.ndarray, loss_func: LossFunction, learning_rate: float=1E-3, batch_size: int=16, epochs: int=32) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Train the multilayer perceptron\n",
    "\n",
    "        :param train_x: full training set input of shape (n x d) n = number of samples, d = number of features\n",
    "        :param train_y: full training set output of shape (n x q) n = number of samples, q = number of outputs per sample\n",
    "        :param val_x: full validation set input\n",
    "        :param val_y: full validation set output\n",
    "        :param loss_func: instance of a LossFunction\n",
    "        :param learning_rate: learning rate for parameter updates\n",
    "        :param batch_size: size of each batch\n",
    "        :param epochs: number of epochs\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x_batches, y_batches = batch_generator(train_x, train_y, batch_size)\n",
    "\n",
    "        training_losses = None\n",
    "        validation_losses = None\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            bi = 0\n",
    "            for bx, by in zip(x_batches, y_batches):\n",
    "                y_pred = self.forward(bx)\n",
    "\n",
    "\n",
    "        return training_losses, validation_losses\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
