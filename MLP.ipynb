{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Vehicle MPG Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows removed: 6\n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "import pandas as pd\n",
    "\n",
    "vehicle_mpg = fetch_ucirepo(id=9)\n",
    "\n",
    "X = vehicle_mpg.data.features\n",
    "y = vehicle_mpg.data.targets\n",
    "\n",
    "data = pd.concat([X, y], axis=1)\n",
    "\n",
    "cleaned_data = data.dropna()\n",
    "\n",
    "X = cleaned_data.iloc[:, :-1]\n",
    "y = cleaned_data.iloc[:, -1]\n",
    "\n",
    "rows_removed = data.shape[0] - cleaned_data.shape[0]\n",
    "print(f\"Rows removed: {rows_removed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting and Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_leftover, y_train, y_leftover = train_test_split(X, y, test_size=0.3, random_state=42, shuffle = True)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_leftover, y_leftover, test_size=0.5, random_state=42, shuffle = True)\n",
    "\n",
    "X_mean = X_train.mean(axis=0)\n",
    "X_std = X_train.std(axis=0)\n",
    "\n",
    "X_train = ((X_train - X_mean) / X_std).to_numpy()\n",
    "X_val = ((X_val - X_mean) / X_std).to_numpy()\n",
    "X_test = ((X_test - X_mean) / X_std).to_numpy()\n",
    "\n",
    "y_mean = y_train.mean()\n",
    "y_std = y_train.std()\n",
    "\n",
    "y_train = ((y_train - y_mean) / y_std).to_numpy().reshape(-1, 1)\n",
    "y_val = ((y_val - y_mean) / y_std).to_numpy().reshape(-1, 1)\n",
    "y_test = ((y_test - y_mean) / y_std).to_numpy().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((274, 7), (274, 1), (59, 7), (59, 1), (59, 7), (59, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #\n",
    "# # This code comes from: https://www.kaggle.com/code/hojjatk/read-mnist-dataset\n",
    "# #\n",
    "# import matplotlib\n",
    "# matplotlib.use('TkAgg')\n",
    "# import numpy as np  # linear algebra\n",
    "# import struct\n",
    "# from array import array\n",
    "# from os.path import join\n",
    "# import random\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# #\n",
    "# # MNIST Data Loader Class\n",
    "# #\n",
    "# class MnistDataloader(object):\n",
    "#     def __init__(self, training_images_filepath, training_labels_filepath,\n",
    "#                  test_images_filepath, test_labels_filepath):\n",
    "#         self.training_images_filepath = training_images_filepath\n",
    "#         self.training_labels_filepath = training_labels_filepath\n",
    "#         self.test_images_filepath = test_images_filepath\n",
    "#         self.test_labels_filepath = test_labels_filepath\n",
    "\n",
    "#     def read_images_labels(self, images_filepath, labels_filepath):\n",
    "#         labels = []\n",
    "#         with open(labels_filepath, 'rb') as file:\n",
    "#             magic, size = struct.unpack(\">II\", file.read(8))\n",
    "#             if magic != 2049:\n",
    "#                 raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "#             labels = array(\"B\", file.read())\n",
    "\n",
    "#         with open(images_filepath, 'rb') as file:\n",
    "#             magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "#             if magic != 2051:\n",
    "#                 raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "#             image_data = array(\"B\", file.read())\n",
    "#         images = []\n",
    "#         for i in range(size):\n",
    "#             images.append([0] * rows * cols)\n",
    "#         for i in range(size):\n",
    "#             img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
    "#             img = img.reshape(28, 28)\n",
    "#             images[i][:] = img\n",
    "\n",
    "#         return images, labels\n",
    "\n",
    "#     def load_data(self):\n",
    "#         x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
    "#         x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
    "#         return (np.array(x_train), np.array(y_train)),(np.array(x_test), np.array(y_test))\n",
    "\n",
    "# #\n",
    "# # Set file paths based on added MNIST Datasets\n",
    "# #\n",
    "# input_path = './data'\n",
    "# training_images_filepath = join(input_path, 'train-images.idx3-ubyte')\n",
    "# training_labels_filepath = join(input_path, 'train-labels-idx1-ubyte')\n",
    "# test_images_filepath = join(input_path, 't10k-images.idx3-ubyte')\n",
    "# test_labels_filepath = join(input_path, 't10k-labels.idx1-ubyte')\n",
    "\n",
    "# #\n",
    "# # Helper function to show a list of images with their relating titles\n",
    "# #\n",
    "# def show_images(images, title_texts):\n",
    "#     cols = 5\n",
    "#     rows = int(len(images)/cols) + 1\n",
    "#     plt.figure(figsize=(30,20))\n",
    "#     index = 1\n",
    "#     for x in zip(images, title_texts):\n",
    "#         image = x[0]\n",
    "#         title_text = x[1]\n",
    "#         plt.subplot(rows, cols, index)\n",
    "#         plt.imshow(image, cmap=plt.cm.gray)\n",
    "#         if (title_text != ''):\n",
    "#             plt.title(title_text, fontsize=15)\n",
    "#         index += 1\n",
    "#     plt.show()\n",
    "\n",
    "# #\n",
    "# # Load MINST dataset\n",
    "# #\n",
    "# mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\n",
    "# (x_train, y_train), (x_test, y_test) = mnist_dataloader.load_data()\n",
    "\n",
    "\n",
    "# np.save('./data/mnist-train-x.npy', x_train.reshape(len(x_train), 784))\n",
    "# np.save('./data/mnist-train-y.npy', y_train)\n",
    "# np.save('./data/mnist-test-x.npy', x_test.reshape(len(x_test), 784))\n",
    "# np.save('./data/mnist-test-y.npy', y_test)\n",
    "\n",
    "# #\n",
    "# # Show some random training and test images\n",
    "# #\n",
    "# images_2_show = []\n",
    "# titles_2_show = []\n",
    "# for i in range(0, 10):\n",
    "#     r = random.randint(1, 60000)\n",
    "#     images_2_show.append(x_train[r])\n",
    "#     titles_2_show.append('training image [' + str(r) + '] = ' + str(y_train[r]))\n",
    "\n",
    "# for i in range(0, 5):\n",
    "#     r = random.randint(1, 10000)\n",
    "#     images_2_show.append(x_test[r])\n",
    "#     titles_2_show.append('test image [' + str(r) + '] = ' + str(y_test[r]))\n",
    "\n",
    "# show_images(images_2_show, titles_2_show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Tuple\n",
    "import math\n",
    "\n",
    "def batch_generator(train_x, train_y, batch_size):\n",
    "    \"\"\"\n",
    "    Generator that yields batches of train_x and train_y.\n",
    "\n",
    "    :param train_x (np.ndarray): Input features of shape (n, f).\n",
    "    :param train_y (np.ndarray): Target values of shape (n, q).\n",
    "    :param batch_size (int): The size of each batch.\n",
    "\n",
    "    :return tuple: (batch_x, batch_y) where batch_x has shape (B, f) and batch_y has shape (B, q). The last batch may be smaller.\n",
    "    \"\"\"\n",
    "    assert len(train_x) == len(train_y), \"Number of samples in X and y do not match.\"\n",
    "\n",
    "    batch_x = np.array_split(train_x, math.ceil(len(train_x) / batch_size), axis=0)\n",
    "    batch_y = np.array_split(train_y, math.ceil(len(train_y) / batch_size), axis=0)\n",
    "    return batch_x, batch_y\n",
    "\n",
    "\n",
    "class ActivationFunction(ABC):\n",
    "    @abstractmethod\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Computes the output of the activation function, evaluated on x\n",
    "\n",
    "        Input args may differ in the case of softmax\n",
    "\n",
    "        :param x (np.ndarray): input\n",
    "        :return: output of the activation function\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Computes the derivative of the activation function, evaluated on x\n",
    "        :param x (np.ndarray): input\n",
    "        :return: activation function's derivative at x\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class Sigmoid(ActivationFunction):\n",
    "\n",
    "    def forward(self, x):\n",
    "        sigma_x = 1 / (1 + np.exp(-x))\n",
    "        return sigma_x\n",
    "    \n",
    "    def derivative(self, x):\n",
    "        sigma_x = self.forward(x)\n",
    "        return sigma_x * (1-sigma_x)           \n",
    "\n",
    "class Tanh(ActivationFunction):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        tanh_x = (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "        return tanh_x\n",
    "    \n",
    "    def derivative(self, x):\n",
    "        tanh_x = self.forward(x)\n",
    "        return 1 - (tanh_x)**2\n",
    "\n",
    "class Relu(ActivationFunction):\n",
    "\n",
    "    def forward(self, x):\n",
    "        return np.max(0, x)\n",
    "    \n",
    "    def derivative(self, x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "    \n",
    "\n",
    "class Softmax(ActivationFunction): \n",
    "\n",
    "    def forward(self, x):\n",
    "        shift_x = x - np.max(x, axis=1, keepdims=True)\n",
    "        exps = np.exp(shift_x)\n",
    "        return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "    \n",
    "    def derivative(self):\n",
    "        raise NotImplementedError(\"Softmax derivative is not implemented\")\n",
    "\n",
    "class Linear(ActivationFunction):\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "    def derivative(self, x):\n",
    "        return np.ones_like(x)\n",
    "\n",
    "class Softplus(ActivationFunction):\n",
    "\n",
    "    def forward(self, x):\n",
    "        return np.log1p(np.exp(x))\n",
    "    \n",
    "    def derivative(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "class Mish(ActivationFunction):\n",
    "\n",
    "    def forward(self, x):\n",
    "        softplus = Softplus()\n",
    "        return x * np.tanh(softplus.forward(x))\n",
    "    \n",
    "    def derivative(self, x):\n",
    "        softplus = Softplus()\n",
    "        tanh_softplus = np.tanh(softplus.forward(x))\n",
    "        sigmoid = Sigmoid.forward(x)\n",
    "        return tanh_softplus + x * sigmoid * (1 - tanh_softplus**2)\n",
    "    \n",
    "\n",
    "class LossFunction(ABC):\n",
    "    @abstractmethod\n",
    "    def loss(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def derivative(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "class SquaredError(LossFunction):\n",
    "\n",
    "    def loss(self, y_true, y_pred):\n",
    "        return 0.5 * np.mean(np.square(y_pred - y_true))\n",
    "    \n",
    "    def derivative(self, y_true, y_pred):\n",
    "        return (y_pred - y_true) / y_true.shape[0]\n",
    "        \n",
    "\n",
    "class CrossEntropy(LossFunction):\n",
    "    def loss(self, y_true, y_pred):\n",
    "        # Clip predictions to avoid log(0) issues\n",
    "        eps = 1e-12\n",
    "        y_pred = np.clip(y_pred, eps, 1. - eps)\n",
    "        # Compute cross-entropy loss averaged over the batch.\n",
    "        return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "    \n",
    "    def derivative(self, y_true, y_pred):\n",
    "        return (y_pred - y_true) / y_true.shape[0]\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, fan_in: int, fan_out: int, activation_function: ActivationFunction, loss_function: LossFunction, dropout_rate: float = 0.0):\n",
    "        \"\"\"\n",
    "        Initializes a layer of neurons\n",
    "\n",
    "        :param fan_in: number of neurons in previous (presynpatic) layer\n",
    "        :param fan_out: number of neurons in this layer\n",
    "        :param activation_function: instance of an ActivationFunction\n",
    "        \"\"\"\n",
    "        self.fan_in = fan_in\n",
    "        self.fan_out = fan_out\n",
    "        self.activation_function = activation_function\n",
    "        self.loss_function = loss_function\n",
    "\n",
    "        # this will store the activations (forward prop)\n",
    "        self.activations = None\n",
    "        # this will store the delta term (dL_dPhi, backward prop)\n",
    "        self.delta = None\n",
    "        self.input = None\n",
    "        self.Z = None\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Initialize weights and biaes\n",
    "        limit = np.sqrt(6 / (self.fan_in + self.fan_out))\n",
    "        self.W = np.random.uniform(-limit, limit, (self.fan_in, self.fan_out)) #weights\n",
    "        # print(self.W.shape)\n",
    "        self.b = np.zeros((1, self.fan_out))  # biases\n",
    "\n",
    "    # def predict(self, x: np.ndarray):\n",
    "    #    return self.W @ x.T + self.b\n",
    "\n",
    "    def forward(self, h: np.ndarray, training: bool = True) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Computes the activations for this layer\n",
    "\n",
    "        :param h: input to layer\n",
    "        :return: layer activations\n",
    "        \"\"\"\n",
    "        self.input = h\n",
    "        self.Z = h @ self.W + self.b\n",
    "        activation = self.activation_function.forward(self.Z)\n",
    "\n",
    "        if training and self.dropout_rate > 0:\n",
    "            keep_prob = 1 - self.dropout_rate\n",
    "            mask = np.random.binomial(1, keep_prob, size=activation.shape) / keep_prob\n",
    "            activation = activation * mask\n",
    "        self.activations = activation\n",
    "        return activation\n",
    "\n",
    "    def backward(self, delta: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Apply backpropagation to this layer and return the weight and bias gradients\n",
    "\n",
    "        :param h: input to this layer\n",
    "        :param delta: delta term from layer above\n",
    "        :return: (weight gradients, bias gradients)\n",
    "         \"\"\"\n",
    "\n",
    "        act_deriv = self.activation_function.derivative(self.Z)\n",
    "        self.delta = np.dot(np.multiply(delta, act_deriv), self.W.T)\n",
    "        dL_dW = np.dot(self.input.T, np.multiply(delta, act_deriv))\n",
    "        dL_db = np.sum(np.multiply(delta, act_deriv))\n",
    "        return dL_dW, dL_db\n",
    "\n",
    "\n",
    "class MultilayerPerceptron:\n",
    "    def __init__(self, layers: Tuple[Layer]):\n",
    "        \"\"\"\n",
    "        Create a multilayer perceptron (densely connected multilayer neural network)\n",
    "        :param layers: list or Tuple of layers\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        This takes the network input and computes the network output (forward propagation)\n",
    "        :param x: network input\n",
    "        :return: network output\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, loss_grad: np.ndarray) -> Tuple[list, list]:\n",
    "        \"\"\"\n",
    "        Applies backpropagation to compute the gradients of the weights and biases for all layers in the network\n",
    "        :param loss_grad: gradient of the loss function\n",
    "        :param input_data: network's input data\n",
    "        :return: (List of weight gradients for all layers, List of bias gradients for all layers)\n",
    "        \"\"\"\n",
    "        dl_dw_all = []\n",
    "        dl_db_all = []\n",
    "        delta = loss_grad\n",
    "\n",
    "        for i, layer in enumerate(reversed(self.layers)):\n",
    "            dl_dw, dl_db = layer.backward(delta)\n",
    "            dl_dw_all.insert(0, dl_dw)\n",
    "            dl_db_all.insert(0, dl_db)\n",
    "            # Only propagate delta if this is not the input layer\n",
    "            if i < len(self.layers) - 1:\n",
    "                delta = layer.delta\n",
    "        return dl_dw_all, dl_db_all\n",
    "\n",
    "    def train(self, train_x: np.ndarray, train_y: np.ndarray, val_x: np.ndarray, val_y: np.ndarray, loss_func: LossFunction, learning_rate: float=1E-3, batch_size: int=16, epochs: int=32) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Train the multilayer perceptron\n",
    "\n",
    "        :param train_x: full training set input of shape (n x d) n = number of samples, d = number of features\n",
    "        :param train_y: full training set output of shape (n x q) n = number of samples, q = number of outputs per sample\n",
    "        :param val_x: full validation set input\n",
    "        :param val_y: full validation set output\n",
    "        :param loss_func: instance of a LossFunction\n",
    "        :param learning_rate: learning rate for parameter updates\n",
    "        :param batch_size: size of each batch\n",
    "        :param epochs: number of epochs\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x_batches, y_batches = batch_generator(train_x, train_y, batch_size)\n",
    "\n",
    "        training_losses = []\n",
    "        validation_losses = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            bi = 0\n",
    "            for bx, by in zip(x_batches, y_batches):\n",
    "                y_pred = self.forward(bx)\n",
    "                # print(\"y_pred shape:\", y_pred.shape, \"y_true shape:\", by.shape)\n",
    "\n",
    "                loss = loss_func.loss(by, y_pred)\n",
    "                total_loss += loss\n",
    "                loss_grad = loss_func.derivative(by, y_pred)\n",
    "                weight_grad, bias_grad = self.backward(loss_grad)\n",
    "                for i, layer in enumerate(self.layers):\n",
    "                    layer.W -= learning_rate * weight_grad[i]\n",
    "                    layer.b -= learning_rate * bias_grad[i]\n",
    "                bi += 1\n",
    "            val_loss = loss_func.loss(val_y, self.forward(val_x))\n",
    "            train_loss = total_loss / len(x_batches)\n",
    "\n",
    "            training_losses.append(train_loss)\n",
    "            validation_losses.append(val_loss)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}  :::  Train Loss={train_loss}  :::  Val Loss={val_loss}\")\n",
    "        return np.array(training_losses), np.array(validation_losses)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  :::  Train Loss=0.7872392750442238  :::  Val Loss=0.6819663289711323\n",
      "Epoch 2  :::  Train Loss=0.5478916906533422  :::  Val Loss=0.5360706169806865\n",
      "Epoch 3  :::  Train Loss=0.4363871830099957  :::  Val Loss=0.45765672789038986\n",
      "Epoch 4  :::  Train Loss=0.37465248362263187  :::  Val Loss=0.40678656754229764\n",
      "Epoch 5  :::  Train Loss=0.33409583674235965  :::  Val Loss=0.36870676413051146\n",
      "Epoch 6  :::  Train Loss=0.30380252011131764  :::  Val Loss=0.3377142206662117\n",
      "Epoch 7  :::  Train Loss=0.2793605816864061  :::  Val Loss=0.31140948458750783\n",
      "Epoch 8  :::  Train Loss=0.25881032307886975  :::  Val Loss=0.28862984604646136\n",
      "Epoch 9  :::  Train Loss=0.24115510163397497  :::  Val Loss=0.2687012844071395\n",
      "Epoch 10  :::  Train Loss=0.22580386424449334  :::  Val Loss=0.25116381798352694\n",
      "Epoch 11  :::  Train Loss=0.2123557719787918  :::  Val Loss=0.2356672281879073\n",
      "Epoch 12  :::  Train Loss=0.2005123804638482  :::  Val Loss=0.22192859494696873\n",
      "Epoch 13  :::  Train Loss=0.1900386519266676  :::  Val Loss=0.20971269738839063\n",
      "Epoch 14  :::  Train Loss=0.18074348741208185  :::  Val Loss=0.1988212527739623\n",
      "Epoch 15  :::  Train Loss=0.1724686114410074  :::  Val Loss=0.189085893980567\n",
      "Epoch 16  :::  Train Loss=0.16508139905965608  :::  Val Loss=0.18036298742376008\n",
      "Epoch 17  :::  Train Loss=0.15846980474526548  :::  Val Loss=0.17252954365599915\n",
      "Epoch 18  :::  Train Loss=0.1525385598173226  :::  Val Loss=0.1654798892652189\n",
      "Epoch 19  :::  Train Loss=0.14720621826780395  :::  Val Loss=0.15912292352453256\n",
      "Epoch 20  :::  Train Loss=0.14240281238891803  :::  Val Loss=0.15337984534817964\n",
      "Epoch 21  :::  Train Loss=0.1380679670306311  :::  Val Loss=0.148182265326013\n",
      "Epoch 22  :::  Train Loss=0.1341493684191351  :::  Val Loss=0.14347063481111516\n",
      "Epoch 23  :::  Train Loss=0.13060151180035398  :::  Val Loss=0.13919293634387497\n",
      "Epoch 24  :::  Train Loss=0.1273846708520211  :::  Val Loss=0.13530358949532742\n",
      "Epoch 25  :::  Train Loss=0.1244640449531934  :::  Val Loss=0.13176253432223606\n",
      "Epoch 26  :::  Train Loss=0.12180905004949746  :::  Val Loss=0.12853446138284913\n",
      "Epoch 27  :::  Train Loss=0.11939272612413393  :::  Val Loss=0.12558816286688754\n",
      "Epoch 28  :::  Train Loss=0.11719123985635099  :::  Val Loss=0.12289598401179656\n",
      "Epoch 29  :::  Train Loss=0.11518346536603374  :::  Val Loss=0.12043335776100944\n",
      "Epoch 30  :::  Train Loss=0.11335062931524906  :::  Val Loss=0.11817840870678532\n",
      "Epoch 31  :::  Train Loss=0.1116760092891051  :::  Val Loss=0.11611161487196993\n",
      "Epoch 32  :::  Train Loss=0.11014467647499954  :::  Val Loss=0.11421551792650596\n"
     ]
    }
   ],
   "source": [
    "mlp = mlp = MultilayerPerceptron([\n",
    "    Layer(fan_in=X_train.shape[1], fan_out=7, activation_function=Linear(), loss_function=SquaredError(), dropout_rate=0.2),\n",
    "    Layer(fan_in=7, fan_out=7, activation_function=Linear(), loss_function=SquaredError(), dropout_rate=0.2),\n",
    "    Layer(fan_in=7, fan_out=1, activation_function=Linear(), loss_function=SquaredError())\n",
    "    # Layer(fan_in=4, fan_out=1, activation_function=Sigmoid(), loss_function=SquaredError())\n",
    "])\n",
    "    \n",
    "training_losses, validation_losses = mlp.train(\n",
    "    train_x=X_train,\n",
    "    train_y=y_train,\n",
    "    val_x=X_val,\n",
    "    val_y=y_val,\n",
    "    loss_func=SquaredError(),\n",
    "    learning_rate=1E-3,\n",
    "    batch_size=16,\n",
    "    epochs=32\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
