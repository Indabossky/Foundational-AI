{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This code comes from: https://www.kaggle.com/code/hojjatk/read-mnist-dataset\n",
    "#\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import numpy as np  # linear algebra\n",
    "import struct\n",
    "from array import array\n",
    "from os.path import join\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#\n",
    "# MNIST Data Loader Class\n",
    "#\n",
    "class MnistDataloader(object):\n",
    "    def __init__(self, training_images_filepath, training_labels_filepath,\n",
    "                 test_images_filepath, test_labels_filepath):\n",
    "        self.training_images_filepath = training_images_filepath\n",
    "        self.training_labels_filepath = training_labels_filepath\n",
    "        self.test_images_filepath = test_images_filepath\n",
    "        self.test_labels_filepath = test_labels_filepath\n",
    "\n",
    "    def read_images_labels(self, images_filepath, labels_filepath):\n",
    "        labels = []\n",
    "        with open(labels_filepath, 'rb') as file:\n",
    "            magic, size = struct.unpack(\">II\", file.read(8))\n",
    "            if magic != 2049:\n",
    "                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "            labels = array(\"B\", file.read())\n",
    "\n",
    "        with open(images_filepath, 'rb') as file:\n",
    "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "            if magic != 2051:\n",
    "                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "            image_data = array(\"B\", file.read())\n",
    "        images = []\n",
    "        for i in range(size):\n",
    "            images.append([0] * rows * cols)\n",
    "        for i in range(size):\n",
    "            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
    "            img = img.reshape(28, 28)\n",
    "            images[i][:] = img\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "    def load_data(self):\n",
    "        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
    "        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
    "        return (np.array(x_train), np.array(y_train)),(np.array(x_test), np.array(y_test))\n",
    "\n",
    "#\n",
    "# Set file paths based on added MNIST Datasets\n",
    "#\n",
    "input_path = '/Users/jmoore/Documents/ML/archive'\n",
    "training_images_filepath = join(input_path, 'train-images.idx3-ubyte')\n",
    "training_labels_filepath = join(input_path, 'train-labels.idx1-ubyte')\n",
    "test_images_filepath = join(input_path, 't10k-images.idx3-ubyte')\n",
    "test_labels_filepath = join(input_path, 't10k-labels.idx1-ubyte')\n",
    "\n",
    "#\n",
    "# Helper function to show a list of images with their relating titles\n",
    "#\n",
    "def show_images(images, title_texts):\n",
    "    cols = 5\n",
    "    rows = int(len(images)/cols) + 1\n",
    "    plt.figure(figsize=(30,20))\n",
    "    index = 1\n",
    "    for x in zip(images, title_texts):\n",
    "        image = x[0]\n",
    "        title_text = x[1]\n",
    "        plt.subplot(rows, cols, index)\n",
    "        plt.imshow(image, cmap=plt.cm.gray)\n",
    "        if (title_text != ''):\n",
    "            plt.title(title_text, fontsize=15)\n",
    "        index += 1\n",
    "    plt.show()\n",
    "\n",
    "#\n",
    "# Load MINST dataset\n",
    "#\n",
    "mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\n",
    "(x_train_init, y_train_init), (x_test_init, y_test_init) = mnist_dataloader.load_data()\n",
    "\n",
    "\n",
    "np.save(join(input_path, 'mnist-train-x.npy'), x_train_init.reshape(len(x_train_init), 784))\n",
    "np.save(join(input_path, 'mnist-train-y.npy'), y_train_init)\n",
    "np.save(join(input_path, 'mnist-test-x.npy'), x_test_init.reshape(len(x_test_init), 784))\n",
    "np.save(join(input_path, 'mnist-test-y.npy'), y_test_init)\n",
    "\n",
    "#\n",
    "# Show some random training and test images\n",
    "#\n",
    "images_2_show = []\n",
    "titles_2_show = []\n",
    "for i in range(0, 10):\n",
    "    r = random.randint(1, 60000)\n",
    "    images_2_show.append(x_train_init[r])\n",
    "    titles_2_show.append('training image [' + str(r) + '] = ' + str(y_train_init[r]))\n",
    "\n",
    "for i in range(0, 5):\n",
    "    r = random.randint(1, 10000)\n",
    "    images_2_show.append(x_test_init[r])\n",
    "    titles_2_show.append('test image [' + str(r) + '] = ' + str(y_test_init[r]))\n",
    "\n",
    "show_images(images_2_show, titles_2_show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  :::  Train Loss=0.32285033555506165  :::  Val Loss=0.19786256182618198\n",
      "Epoch 2  :::  Train Loss=0.17133560511403373  :::  Val Loss=0.15756030233568524\n",
      "Epoch 3  :::  Train Loss=0.14505447309507105  :::  Val Loss=0.19035578663443184\n",
      "Epoch 4  :::  Train Loss=0.13460801643849196  :::  Val Loss=0.19163586276245018\n",
      "Epoch 5  :::  Train Loss=0.13659909560182767  :::  Val Loss=0.1810042131074192\n",
      "Epoch 6  :::  Train Loss=0.13922604486537735  :::  Val Loss=0.1918326876673106\n",
      "Epoch 7  :::  Train Loss=0.1309579631098805  :::  Val Loss=0.2084296153638575\n",
      "Epoch 8  :::  Train Loss=0.13621430282837657  :::  Val Loss=0.2230608281953006\n",
      "Epoch 9  :::  Train Loss=0.13352388357379555  :::  Val Loss=0.260152428183953\n",
      "Epoch 10  :::  Train Loss=0.12984692332134615  :::  Val Loss=0.22091328896487267\n",
      "Epoch 11  :::  Train Loss=0.12834361796983246  :::  Val Loss=0.25618463722244755\n",
      "Epoch 12  :::  Train Loss=0.13426228758586697  :::  Val Loss=0.23959321400433414\n",
      "Epoch 13  :::  Train Loss=0.1333487287069731  :::  Val Loss=0.21767726160297957\n",
      "Epoch 14  :::  Train Loss=0.13649722692435984  :::  Val Loss=0.2811278854946086\n",
      "Epoch 15  :::  Train Loss=0.13316946496547843  :::  Val Loss=0.25277435674804916\n",
      "Epoch 16  :::  Train Loss=0.14055957754672907  :::  Val Loss=0.24383315890674578\n",
      "Epoch 17  :::  Train Loss=0.14096622597753108  :::  Val Loss=0.28008438268486574\n",
      "Epoch 18  :::  Train Loss=0.1223961099935918  :::  Val Loss=0.29656006593583895\n",
      "Epoch 19  :::  Train Loss=0.13155200690748933  :::  Val Loss=0.311112902610987\n",
      "Epoch 20  :::  Train Loss=0.13604011305655608  :::  Val Loss=0.29358534395110536\n",
      "Epoch 21  :::  Train Loss=0.13924906784967384  :::  Val Loss=0.2730427106513549\n",
      "Epoch 22  :::  Train Loss=0.13676709045512353  :::  Val Loss=0.2938194544712206\n",
      "Epoch 23  :::  Train Loss=0.1348232658840325  :::  Val Loss=0.28907916942390244\n",
      "Epoch 24  :::  Train Loss=0.1270212485359831  :::  Val Loss=0.303851257850605\n",
      "Epoch 25  :::  Train Loss=0.12178033958776749  :::  Val Loss=0.3306020118030665\n",
      "Epoch 26  :::  Train Loss=0.13670685307077524  :::  Val Loss=0.35730134421249904\n",
      "Epoch 27  :::  Train Loss=0.1355498208267573  :::  Val Loss=0.324031261961769\n",
      "Epoch 28  :::  Train Loss=0.13283018912560607  :::  Val Loss=0.31843117647896596\n",
      "Epoch 29  :::  Train Loss=0.14072390758644704  :::  Val Loss=0.2989914145471756\n",
      "Epoch 30  :::  Train Loss=0.12742032551328564  :::  Val Loss=0.3191632926071426\n",
      "Epoch 31  :::  Train Loss=0.12743872041965848  :::  Val Loss=0.33051812579998463\n",
      "Epoch 32  :::  Train Loss=0.13775572621824564  :::  Val Loss=0.2944808132398849\n",
      "Test Accuracy: 97.92%\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from MLP import MultilayerPerceptron, Layer, Mish, Softplus, CrossEntropy, Relu, Softmax, SquaredError\n",
    "\n",
    "def one_hot_encode(y: np.ndarray, num_classes: int) -> np.ndarray:\n",
    "    encoded = np.zeros((y.shape[0], num_classes))\n",
    "    for i, label in enumerate(y):\n",
    "        encoded[i, int(label)] = 1\n",
    "    return encoded\n",
    "\n",
    "# Flatten images and scale pixel values to [0,1]\n",
    "x_train_flat = x_train_init.reshape(x_train_init.shape[0], -1) / 255.0\n",
    "x_test_flat = x_test_init.reshape(x_test_init.shape[0], -1) / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding (10 classes)\n",
    "y_train_onehot = one_hot_encode(y_train_init, 10)\n",
    "y_test_onehot = one_hot_encode(y_test_init, 10)\n",
    "\n",
    " # Split the original training set into 80% training and 20% validation\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_flat, y_train_onehot, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "mlp = MultilayerPerceptron([\n",
    "    # Input layer: 784 features -> Hidden layer with 128 neurons (ReLU) with dropout 20%\n",
    "    Layer(fan_in=784, fan_out=256, activation_function=Relu(), dropout_rate=0.2),\n",
    "    # Hidden layer: 128 -> 64 neurons (ReLU) with dropout 20%\n",
    "    Layer(fan_in=256, fan_out=128, activation_function=Relu(), dropout_rate=0.2),\n",
    "    # Output layer: 64 -> 10 neurons with Softmax activation (for 10 classes)\n",
    "    Layer(fan_in=128, fan_out=10, activation_function=Softmax())\n",
    "])\n",
    "\n",
    "\n",
    "# Train the MLP\n",
    "\n",
    "training_losses, validation_losses = mlp.train(\n",
    "    train_x=x_train,\n",
    "    train_y=y_train,\n",
    "    val_x=x_val,\n",
    "    val_y=y_val,\n",
    "    loss_func=CrossEntropy(),\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=32,\n",
    "    epochs=32,\n",
    "    use_rmsprop=True,\n",
    "    decay_rate=0.9,\n",
    "    epsilon=1e-8\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate the Model on the Test Set and Report Accuracy\n",
    "\n",
    "y_test_pred = np.argmax(mlp.forward(x_test_flat, training=False), axis=1)\n",
    "# y_test contains the true labels (as integers)\n",
    "test_accuracy = np.mean(y_test_pred == np.array(y_test_init))\n",
    "print(f\"Test Accuracy: {test_accuracy * 100}%\")\n",
    "\n",
    "\n",
    "plt.plot(training_losses, label=\"Training Loss\")\n",
    "plt.plot(validation_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"MNIST Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume:\n",
    "# - x_test_flat is your test data of shape (n_samples, 784) normalized to [0,1]\n",
    "# - y_test_init contains the true labels as integers\n",
    "# - mlp is your trained MultilayerPerceptron\n",
    "\n",
    "# Select one sample per class based on the true labels\n",
    "selected_indices = {}\n",
    "for idx, true_label in enumerate(y_test_init):\n",
    "    if true_label not in selected_indices:\n",
    "        selected_indices[true_label] = idx\n",
    "    if len(selected_indices) == 10:\n",
    "        break\n",
    "\n",
    "# Convert the selected indices to a sorted list (by digit)\n",
    "selected_indices = [selected_indices[digit] for digit in sorted(selected_indices.keys())]\n",
    "\n",
    "# Get the corresponding test images and reshape them to 28x28 for display\n",
    "selected_images = x_test_flat[selected_indices].reshape(-1, 28, 28)\n",
    "\n",
    "# Get the predictions for the selected samples\n",
    "selected_preds = np.argmax(mlp.forward(x_test_flat[selected_indices], training=False), axis=1)\n",
    "\n",
    "# Plot the images with predicted labels\n",
    "import matplotlib.pyplot as plt\n",
    "cols = 5\n",
    "rows = 2\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, idx in enumerate(selected_indices):\n",
    "    plt.subplot(rows, cols, i+1)\n",
    "    plt.imshow(selected_images[i], cmap='gray')\n",
    "    plt.title(f\"Pred: {selected_preds[i]}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
